## Multi-Modal Music Emotion Classification with LSTMs

To improve music emotion classification, this work combines
audio and lyric data using a multimodal approach. Audio 
features and Word2Vec-based lyric vectors are processed 
through an LSTM model with attention. The model achieves 
higher accuracy than unimodal methods, with a low MAE of 0.089.

*The source code was written by Nextinvitation from GitHub.

